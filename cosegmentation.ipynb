{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SibdvxA-BPtb"
   },
   "source": [
    "# Deep ViT Features - Co-segmentation\n",
    "Given a set of images, segment all the common objects among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "collapsed": true,
    "id": "wlEqPSu6A2Lk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/r/ramzie/dev/dino-vit-features\n"
     ]
    }
   ],
   "source": [
    "#@title Installations and mounting\n",
    "# !pip install tqdm\n",
    "# !pip install faiss-cpu\n",
    "# !pip install timm\n",
    "# !pip install opencv-python\n",
    "# !pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n",
    "# !git clone https://github.com/ShirAmir/dino-vit-features.git\n",
    "# import sys\n",
    "# sys.path.append('dino-vit-features')\n",
    "%cd ~/dev/dino-vit-features\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQELHecYBuI_"
   },
   "source": [
    "## Change Runtime Type\n",
    "To get a GPU in Google Colab, go to the top menu: Runtime ➔ Change runtime type and select GPU as Hardware accelerator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "id": "CXYGv2mxA2Ll",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#@title Configuration:\n",
    "#@markdown Choose image paths:\n",
    "images_paths = ['dino-vit-features/images/cat.jpg', 'dino-vit-features/images/ibex.jpg'] #@param\n",
    "images_paths = ['images/cat.jpg', 'images/ibex.jpg'] #@param\n",
    "#@markdown Choose loading size:\n",
    "load_size = 360 #@param\n",
    "#@markdown Choose layer of descriptor:\n",
    "layer = 11 #@param\n",
    "#@markdown Choose facet of descriptor:\n",
    "facet = 'key' #@param\n",
    "#@markdown Choose if to use a binned descriptor:\n",
    "bin=False #@param\n",
    "#@markdown Choose fg / bg threshold:\n",
    "thresh=0.065 #@param\n",
    "#@markdown Choose model type:\n",
    "model_type='vit_small_patch8_224' #@param\n",
    "#@markdown Choose stride:\n",
    "stride=4 #@param\n",
    "#@markdown Choose elbow coefficient for setting number of clusters\n",
    "elbow=0.975 #@param\n",
    "#@markdown Choose percentage of votes to make a cluster salient.\n",
    "votes_percentage=75 #@param\n",
    "#@markdown Choose whether to remove outlier images\n",
    "remove_outliers=False #@param\n",
    "#@markdown Choose threshold to distinguish inliers from outliers\n",
    "outliers_thresh=0.7 #@param\n",
    "#@markdown Choose interval for sampling descriptors for training\n",
    "sample_interval=100 #@param\n",
    "#@markdown Use low resolution saliency maps -- reduces RAM usage.\n",
    "low_res_saliency_maps=True #@param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "YylGkwBEA2Lm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for //: 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcosegmentation\u001b[39;00m \u001b[39mimport\u001b[39;00m find_cosegmentation, draw_cosegmentation, draw_cosegmentation_binary_masks\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m      \u001b[39m# computing cosegmentation\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     seg_masks, pil_images \u001b[39m=\u001b[39m find_cosegmentation(images_paths, elbow, load_size, layer, facet, \u001b[39mbin\u001b[39;49m, thresh, model_type,\n\u001b[1;32m      9\u001b[0m                                                 stride, votes_percentage, sample_interval, remove_outliers,\n\u001b[1;32m     10\u001b[0m                                                 outliers_thresh, low_res_saliency_maps)\n\u001b[1;32m     12\u001b[0m     figs, axes \u001b[39m=\u001b[39m [], []\n\u001b[1;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m pil_image \u001b[39min\u001b[39;00m pil_images:\n",
      "File \u001b[0;32m~/dev/dino-vit-features/cosegmentation.py:41\u001b[0m, in \u001b[0;36mfind_cosegmentation\u001b[0;34m(image_paths, elbow, load_size, layer, facet, bin, thresh, model_type, stride, votes_percentage, sample_interval, remove_outliers, outliers_thresh, low_res_saliency_maps, save_dir)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mfinding cosegmentation of a set of images.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m:param image_paths: a list of paths of all the images.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m:return: a list of segmentation masks and a list of processed pil images.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 41\u001b[0m extractor \u001b[39m=\u001b[39m ViTExtractor(model_type, stride, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     42\u001b[0m descriptors_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     43\u001b[0m saliency_maps_list \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/dev/dino-vit-features/extractor.py:43\u001b[0m, in \u001b[0;36mViTExtractor.__init__\u001b[0;34m(self, model_type, stride, model, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m ViTExtractor\u001b[39m.\u001b[39mcreate_model(model_type)\n\u001b[0;32m---> 43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m ViTExtractor\u001b[39m.\u001b[39;49mpatch_vit_resolution(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, stride\u001b[39m=\u001b[39;49mstride)\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/dev/dino-vit-features/extractor.py:139\u001b[0m, in \u001b[0;36mViTExtractor.patch_vit_resolution\u001b[0;34m(model, stride)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m    138\u001b[0m stride \u001b[39m=\u001b[39m nn_utils\u001b[39m.\u001b[39m_pair(stride)\n\u001b[0;32m--> 139\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m([(patch_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m s_) \u001b[39m*\u001b[39m s_ \u001b[39m==\u001b[39m patch_size \u001b[39mfor\u001b[39;00m s_ \u001b[39min\u001b[39;00m\n\u001b[1;32m    140\u001b[0m             stride]), \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstride \u001b[39m\u001b[39m{\u001b[39;00mstride\u001b[39m}\u001b[39;00m\u001b[39m should divide patch_size \u001b[39m\u001b[39m{\u001b[39;00mpatch_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[39m# fix the stride\u001b[39;00m\n\u001b[1;32m    143\u001b[0m model\u001b[39m.\u001b[39mpatch_embed\u001b[39m.\u001b[39mproj\u001b[39m.\u001b[39mstride \u001b[39m=\u001b[39m stride\n",
      "File \u001b[0;32m~/dev/dino-vit-features/extractor.py:139\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[1;32m    138\u001b[0m stride \u001b[39m=\u001b[39m nn_utils\u001b[39m.\u001b[39m_pair(stride)\n\u001b[0;32m--> 139\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m([(patch_size \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m s_) \u001b[39m*\u001b[39m s_ \u001b[39m==\u001b[39m patch_size \u001b[39mfor\u001b[39;00m s_ \u001b[39min\u001b[39;00m\n\u001b[1;32m    140\u001b[0m             stride]), \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstride \u001b[39m\u001b[39m{\u001b[39;00mstride\u001b[39m}\u001b[39;00m\u001b[39m should divide patch_size \u001b[39m\u001b[39m{\u001b[39;00mpatch_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[39m# fix the stride\u001b[39;00m\n\u001b[1;32m    143\u001b[0m model\u001b[39m.\u001b[39mpatch_embed\u001b[39m.\u001b[39mproj\u001b[39m.\u001b[39mstride \u001b[39m=\u001b[39m stride\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for //: 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from cosegmentation import find_cosegmentation, draw_cosegmentation, draw_cosegmentation_binary_masks\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "     # computing cosegmentation\n",
    "    seg_masks, pil_images = find_cosegmentation(images_paths, elbow, load_size, layer, facet, bin, thresh, model_type,\n",
    "                                                stride, votes_percentage, sample_interval, remove_outliers,\n",
    "                                                outliers_thresh, low_res_saliency_maps)\n",
    "\n",
    "    figs, axes = [], []\n",
    "    for pil_image in pil_images:\n",
    "      fig, ax = plt.subplots()\n",
    "      ax.axis('off')\n",
    "      ax.imshow(pil_image)\n",
    "      figs.append(fig)\n",
    "      axes.append(ax)\n",
    "    \n",
    "    # saving cosegmentations\n",
    "    binary_mask_figs = draw_cosegmentation_binary_masks(seg_masks)\n",
    "    chessboard_bg_figs = draw_cosegmentation(seg_masks, pil_images)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convit_base',\n",
       " 'convit_small',\n",
       " 'convit_tiny',\n",
       " 'vit_base_r26_s32_224',\n",
       " 'vit_base_r50_s16_224',\n",
       " 'vit_base_r50_s16_224_in21k',\n",
       " 'vit_base_r50_s16_384',\n",
       " 'vit_base_resnet26d_224',\n",
       " 'vit_base_resnet50_224_in21k',\n",
       " 'vit_base_resnet50_384',\n",
       " 'vit_base_resnet50d_224',\n",
       " 'vit_large_r50_s32_224',\n",
       " 'vit_large_r50_s32_224_in21k',\n",
       " 'vit_large_r50_s32_384',\n",
       " 'vit_small_r26_s32_224',\n",
       " 'vit_small_r26_s32_224_in21k',\n",
       " 'vit_small_r26_s32_384',\n",
       " 'vit_small_resnet26d_224',\n",
       " 'vit_small_resnet50d_s16_224',\n",
       " 'vit_tiny_r_s16_p8_224',\n",
       " 'vit_tiny_r_s16_p8_224_in21k',\n",
       " 'vit_tiny_r_s16_p8_384']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('*vi*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.12'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (0.4.12)\n",
      "Collecting timm\n",
      "  Using cached timm-0.6.12-py3-none-any.whl (549 kB)\n",
      "Requirement already satisfied: pyyaml in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: torchvision in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from timm) (0.11.2)\n",
      "Requirement already satisfied: torch>=1.7 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from timm) (1.10.1)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
      "Requirement already satisfied: typing_extensions in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from torch>=1.7->timm) (3.10.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from huggingface-hub->timm) (4.64.1)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from huggingface-hub->timm) (21.3)\n",
      "Collecting requests\n",
      "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: numpy in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from torchvision->timm) (1.21.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from torchvision->timm) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: charset-normalizer, urllib3, filelock, requests, huggingface-hub, timm\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 0.4.12\n",
      "    Uninstalling timm-0.4.12:\n",
      "      Successfully uninstalled timm-0.4.12\n",
      "Successfully installed charset-normalizer-3.0.1 filelock-3.9.0 huggingface-hub-0.12.0 requests-2.28.2 timm-0.6.12 urllib3-1.26.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(8, 8), stride=(8, 8))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.create_model('vit_small_patch8_224.dino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rwightman/pytorch-image-models.git\n",
      "  Cloning https://github.com/rwightman/pytorch-image-models.git to /tmp/pip-req-build-vux6nwej\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/rwightman/pytorch-image-models.git /tmp/pip-req-build-vux6nwej\n",
      "  Resolved https://github.com/rwightman/pytorch-image-models.git to commit 709d5e0d9d2d3f501531506eda96a435737223a3\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from timm==0.8.11.dev0) (1.10.1)\n",
      "Requirement already satisfied: torchvision in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from timm==0.8.11.dev0) (0.11.2)\n",
      "Requirement already satisfied: pyyaml in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from timm==0.8.11.dev0) (6.0)\n",
      "Requirement already satisfied: huggingface_hub in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from timm==0.8.11.dev0) (0.12.0)\n",
      "Requirement already satisfied: typing_extensions in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from torch>=1.7->timm==0.8.11.dev0) (3.10.0.2)\n",
      "Requirement already satisfied: filelock in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from huggingface_hub->timm==0.8.11.dev0) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from huggingface_hub->timm==0.8.11.dev0) (21.3)\n",
      "Requirement already satisfied: requests in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from huggingface_hub->timm==0.8.11.dev0) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from huggingface_hub->timm==0.8.11.dev0) (4.64.1)\n",
      "Requirement already satisfied: numpy in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from torchvision->timm==0.8.11.dev0) (1.21.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from torchvision->timm==0.8.11.dev0) (8.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from packaging>=20.9->huggingface_hub->timm==0.8.11.dev0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from requests->huggingface_hub->timm==0.8.11.dev0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from requests->huggingface_hub->timm==0.8.11.dev0) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from requests->huggingface_hub->timm==0.8.11.dev0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /users/r/ramzie/anaconda3/envs/dino-vit-feats-env/lib/python3.9/site-packages (from requests->huggingface_hub->timm==0.8.11.dev0) (3.4)\n",
      "Building wheels for collected packages: timm\n",
      "  Building wheel for timm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for timm: filename=timm-0.8.11.dev0-py3-none-any.whl size=1989971 sha256=0351e91465ca08f74148d45d2265012f45ec76940377b04e3a3ae82064ec6ed5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fkv1dkb_/wheels/eb/1e/79/4dfc1bba276172378ab3e51ceed8e1e59ff8fba24e453244bd\n",
      "Successfully built timm\n",
      "Installing collected packages: timm\n",
      "  Attempting uninstall: timm\n",
      "    Found existing installation: timm 0.6.12\n",
      "    Uninstalling timm-0.6.12:\n",
      "      Successfully uninstalled timm-0.6.12\n",
      "Successfully installed timm-0.8.11.dev0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/rwightman/pytorch-image-models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "cosegmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
